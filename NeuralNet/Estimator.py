"""
Created on Thu 22 May 2014 at 09:25

@author: WLZ

A modular neural network library with the following flexibilities:
	- Number of layers
	- Connection structures
	- Transfer functions (linear, logistic, tanh)
	- Layer functions (e.g. MSE, sparsity penalty)
	- Param functions (e.g. decay weight)

And the following functionalities:
	- Greedy layer-wise training
	- Fine-tuning
	- Hyperparameter search
	- Normalization
	- Display
	- Saving and loading

This is based on work by:
	- SW, DC: Original SAE
	- OCC: Inheritance from sklearn; object oriented framework
	- WY: stacked SAE; new connection structures; greedy layer-wise training
	- Neurolab: code structure and organization

All optimization will be carried out by scipy.optimize.fmin_l_bfgs_b .

If desired, we can use BaseEstimator from sklearn.base as the parent for Net. However, "object" is good enough for our purposes.
"""

from __future__ import division
import numpy as np
from scipy.optimize import fmin_l_bfgs_b as minimize
import time
import Error
import Namer
#from sklearn.base import BaseEstimator


class Net(object):
    
    def __init__():
          

    def fit(self,data,status=False,factr=1e7,maxiter=100,random_seed=None):
        """ this function will initialize and train the weights of the 
            sparse autoencoder. Recall that the sparse autoencoder is
            actually a 3-layers neural network.
            
        Parameters
        ----------
        data : array of shape (n_samples, visible_size)
            Data matrix.

        verbose : boolean, True by default
            Whether to print status during optimization.

        factr : float
            The iteration stops when 
            (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps,
            where eps is the machine precision, which is automatically 
            generated by the code. Typical values for factr are: 
            1e12 for low accuracy; 1e7 for moderate accuracy;
            10.0 for extremely high accuracy.
        """
                
        self.W1,self.W2,self.b1,self.b2,self.D,cost,messages = \
            sae_learning(data,self.hidden_size,self.sparse_rate,
                         self.sparse_weight,self.decay_weight,
                         W1_init=self.W1,W2_init=self.W2,
                         b1_init=self.b1,b2_init=self.b2,
                         D_init =self.D, factr=factr,maxiter=maxiter)
        
        if status:
            if messages['warnflag']==0:
                converged = True
            else:
                converged = False
            return self,cost,converged
        else:
            return self
        
    def transform(self,samples):
        """ this function returns the first layer activations
            of the input (samples)
        """
        #return forward_pass_scaled_conditional(samples,self.W1,self.b1,self.D)
        return forward_pass_conditional(samples,self.W1,self.b1,self.D)
    
    def predict(self,samples):
        """ this function returns the second layer activations (estimates)
            of the input (samples)
        """        
        return np.dot(self.transform(samples),self.W2) + self.b2
    
    def features(self):
        return self.W1.T

    def score(self,data):
        theta = ravel_params(self.W1,self.W2,self.b1,self.b2,self.D)
        return costgrad(theta,self.hidden_size,self.sparse_rate,
                        self.sparse_weight,self.decay_weight,
                        data,cost_only=True)

class Layer(object):
    """
    Abstract Neural Layer class

    :Parameters:
        size_in: int
            Number of inputs (neurons in previous layer)
        size_out: int
            Number of outputs (neurons in current layer)        
        transfer: function
            transfer function from Transfer
        number: int
        	Position of current layer in a neural network (optional; for error reporting)
    	partition_in, partition_out: list
    		List of tuples indicating sublayers.

    """

class Sublayer(object):
    """
    Abstract Neural Layer class. Basic building block of Layer.

    :Parameters:
        size_in: int
            Number of inputs (neurons in previous layer)
        size_out: int
            Number of outputs (neurons in current layer)        
        transfer: str
            transfer function from Transfer
            examples: "linear","tanh","logistic"
        depth: int
        	depth of the layer it belongs to 
        	(optional; for error reporting)
    	position: int
    		position of this sublayer within its parent layer 
    		(optional; for error reporting)
    """	

    def __init__(self, size_in, size_out, transfer, depth = None, position = None):
    	self.id = Namer.sublayer_name(depth,position)
    	
    	self.loc = [depth,position]
    	self.size_in = size_in
    	self.size_out = size_out
    	self.w_mat = np.zeros((self.size_in, self.size_out))
    	self.b_vec = np.zeros(self.size_out)
    	
    	self.transfer = Transfer.assign(transfer, self.id)		

		    	

    def feed_forward(self, data):
        """ Layer simulation step """
        assert len(inp) == self.ci
        out = self._step(inp)
        self.inp = inp
        self.out = out

    def feed_backward(self, data):
    	""" Backpropagation step """

    def init(self):
        """ Init Layer random values """
        if type(self.initf) is list:
            for initf in self.initf:
                initf(self)
        elif self.initf is not None:
            self.initf(self)

