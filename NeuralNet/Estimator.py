"""
Created on Thu 22 May 2014 at 09:25

@author: WLZ

A modular neural network library with the following flexibilities:
	- Number of layers
	- Connection structures
	- Transfer functions (linear, logistic, tanh)
	- Layer functions (e.g. MSE, sparsity penalty)
	- Param functions (e.g. decay weight)

And the following functionalities:
	- Greedy layer-wise training
	- Fine-tuning
	- Hyperparameter search
	- Normalization
	- Display
	- Saving and loading

This is based on work by:
	- SW, DC: Original SAE
	- OCC: Inheritance from sklearn; object oriented framework
	- WY: stacked SAE; new connection structures; greedy layer-wise training
	- Neurolab: code structure and organization

All optimization will be carried out by scipy.optimize.fmin_l_bfgs_b .

If desired, we can use BaseEstimator from sklearn.base as the parent for Net. However, "object" is good enough for our purposes.
"""

from __future__ import division
import numpy as np
from scipy.optimize import fmin_l_bfgs_b as minimize
import time
import Error
import Namer
import Transfer
import Init
#from sklearn.base import BaseEstimator


class Net(object):
    
    def __init__():
          

    def fit(self,data,status=False,factr=1e7,maxiter=100,random_seed=None):
        """ this function will initialize and train the weights of the 
            sparse autoencoder. Recall that the sparse autoencoder is
            actually a 3-layers neural network.
            
        Parameters
        ----------
        data : array of shape (n_samples, visible_size)
            Data matrix.

        verbose : boolean, True by default
            Whether to print status during optimization.

        factr : float
            The iteration stops when 
            (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps,
            where eps is the machine precision, which is automatically 
            generated by the code. Typical values for factr are: 
            1e12 for low accuracy; 1e7 for moderate accuracy;
            10.0 for extremely high accuracy.
        """
                
        self.W1,self.W2,self.b1,self.b2,self.D,cost,messages = \
            sae_learning(data,self.hidden_size,self.sparse_rate,
                         self.sparse_weight,self.decay_weight,
                         W1_init=self.W1,W2_init=self.W2,
                         b1_init=self.b1,b2_init=self.b2,
                         D_init =self.D, factr=factr,maxiter=maxiter)
        
        if status:
            if messages['warnflag']==0:
                converged = True
            else:
                converged = False
            return self,cost,converged
        else:
            return self
        
    def transform(self,samples):
        """ this function returns the first layer activations
            of the input (samples)
        """
        #return forward_pass_scaled_conditional(samples,self.W1,self.b1,self.D)
        return forward_pass_conditional(samples,self.W1,self.b1,self.D)
    
    def predict(self,samples):
        """ this function returns the second layer activations (estimates)
            of the input (samples)
        """        
        return np.dot(self.transform(samples),self.W2) + self.b2
    
    def features(self):
        return self.W1.T

    def score(self,data):
        theta = ravel_params(self.W1,self.W2,self.b1,self.b2,self.D)
        return costgrad(theta,self.hidden_size,self.sparse_rate,
                        self.sparse_weight,self.decay_weight,
                        data,cost_only=True)

class Layer(object):
    """
    Abstract Neural Layer class

    :Parameters:
        size_in: int
            Number of inputs (neurons in previous layer)
        size_out: int
            Number of outputs (neurons in current layer)        
        transfer: function
            transfer function from Transfer
        number: int
        	Position of current layer in a neural network (optional; for error reporting)
    	partition_in, partition_out: list
    		List of tuples indicating sublayers.

    """
    def init(self):
        """ Init Layer random values """
        if type(self.initf) is list:
            for initf in self.initf:
                initf(self)
        elif self.initf is not None:
            self.initf(self)


class Sublayer(object):
    """
    Abstract Neural Layer class. Basic building block of Layer.

    :Parameters:
        transfer: str
            transfer function from Transfer
            examples: "linear","tanh","logistic"
        w_mat: array
            weight matrix of shape (M,N) 
        b_vec: array
            bias vector of shape (N,)         
        depth: int
        	depth of the layer it belongs to 
        	(optional; for error reporting)
    	position: int
    		position of this sublayer within its parent layer 
    		(optional; for error reporting)

    """	

    def __init__(self, transfer, w_mat, b_vec, depth = None, position = None):    	
        # Sublayer identification attributes
        self.id = Namer.sublayer_name(depth,position)    	
    	self.depth = depth
        self.position = position

        # Sublayer parameter attributes
    	self.size_in, self.size_out = np.shape(w_mat)
        if np.shape(b_vec) != (self.size_out,):
            raise Error.InitError("Param dimension mismatch @ " + self.id)
    	self.w_mat = w_mat
    	self.b_vec = b_vec
    	
        # Sublayer transfer function
    	try:
            self.transfer = Transfer.assign(transfer)
        except NameError as error:
            raise Error.InitError(error.message + " in transfer func. assignment @ " + self.id)

    def feed_forward(self, data_in):
        """ Feed forward step """        
        func_name = "feed_forward"
        try:
            return self.transfer(np.dot(data_in,self.w_mat)+b_vec)    
        except Exception as error:
            raise Error.EvalError(error.message + " in execution of " + func_name + " @ " + self.id)                      

    def backprop(self, delta, data_in = None, data_out = None):
        """ Backpropagation of deltas """
        func_name = "backprop"
        try:
            return np.dot(delta*self.transfer.deriv(data_in,data_out), self.w_mat.T)
        except Exception as error:
            raise Error.EvalError(error.message + " in execution of " + func_name + " @ " + self.id)              

    def deriv_w(self, delta, data_in = None, data_out = None):
        """ Derivative w.r.t. w_mat """        
        func_name = "deriv_w"
        try:
            return np.dot(data_in.T,delta*self.transfer.deriv(data_in,data_out))
        except Exception as error:
            raise Error.EvalError(error.message + " in execution of " + func_name + " @ " + self.id)

    def deriv_b(self, delta, data_in = None, data_out = None):
        """ Derivative w.r.t. b_vec """
        func_name = "deriv_b"
        try:
            return np.sum(delta*self.transfer.deriv(data_in,data_out))
        except Exception as error:
            raise Error.EvalError(error.message + " in execution of " + func_name + " @ " + self.id)